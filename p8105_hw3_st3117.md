p8105\_hw2\_st3117
================
Sha Tao
October 6, 2018

Problem 1\_1 BRFSS data cleaning
--------------------------------

``` r
brfss = 
  brfss_smart2010 %>% 
  janitor::clean_names() %>%
  filter(topic == "Overall Health") %>% 
  separate(locationdesc, into = c("remove", "county"), sep = 5) %>% 
  select(year, state = locationabbr, county, response, data_value, everything(), -remove) %>% 
  mutate(response = as.factor(response),
         response = factor(response, levels = c("Excellent", "Very good", "Good", "Fair", "Poor")))

# check the factor level of response

levels(brfss$response) %>% 
  knitr::kable()
```

| x         |
|:----------|
| Excellent |
| Very good |
| Good      |
| Fair      |
| Poor      |

Problem 1\_2 In 2002, which states were observed at 7 locations?
----------------------------------------------------------------

``` r
# check number of locations distribution in 2002

brfss %>% 
  filter(year == 2002) %>% 
  group_by(state) %>% 
  summarize(n = n_distinct(county)) %>% 
  ggplot(aes(x = reorder(state, n), y = n)) +
    geom_point() +
    labs(
      title = "Number of Locations Plot",
      x = "States",
      y = "Number of Locationss"
      ) +
    theme(axis.text.x = element_text(size = 7, hjust = 1, angle = 45))
```

<img src="p8105_hw3_st3117_files/figure-markdown_github/Problem 1_2-1.png" width="90%" />

``` r
# check mean of the number of locations distribution in 2002

brfss %>% 
  filter(year == 2002) %>% 
  group_by(state) %>% 
  summarize(n = n_distinct(county)) %>% 
  summarize(mean_n = mean(n, na.rm = TRUE))
```

    ## # A tibble: 1 x 1
    ##   mean_n
    ##    <dbl>
    ## 1   3.20

``` r
# find states which were observed at 7 locations in 2002

brfss %>% 
  filter(year == 2002) %>% 
  group_by(state) %>% 
  summarize(n = n_distinct(county)) %>% 
  filter(n == 7) %>% 
  knitr::kable()
```

| state |    n|
|:------|----:|
| CT    |    7|
| FL    |    7|
| NC    |    7|

Connecticut, Florida and North Carolina were observed at 7 locations in 2002.
Among all the states, they ranked 4th from the highest observations which are Pennsylvania (10).
The mean number of locations are 3.2.

Problem 1\_3 Spaghetti plot showing the number of observations in each state from 2002 to 2010s
-----------------------------------------------------------------------------------------------

``` r
# spaghetti plot

brfss %>%
  group_by(year, state) %>% 
  summarize(n = n_distinct(county)) %>% 
  ggplot(aes(x = year, y = n, color = state)) +
    geom_line() +
    labs(
      title = "Number of Observations Plot",
      x = "Year",
      y = "Number of Observations"
      ) +
    theme(legend.position = "right")
```

<img src="p8105_hw3_st3117_files/figure-markdown_github/Problem 1_3-1.png" width="90%" />

``` r
# check the outliers

brfss %>%
  group_by(year, state) %>% 
  summarize(n = n_distinct(county)) %>% 
  arrange(-n) %>% 
  head(3) %>% 
  knitr::kable()
```

|  year| state |    n|
|-----:|:------|----:|
|  2007| FL    |   44|
|  2010| FL    |   41|
|  2005| NJ    |   19|

From the spaghetti plot, we can see most of the states had relatively stable number of ovservations across years, from 0 to 20. There were two peaks at 2007 and 2010 for Florida. Comparing with the table, we see that the observations in 2007 reached 44 and it reached 41 in 2010.

Problem 1\_4 NY State “Excellent” responses
-------------------------------------------

``` r
brfss %>%
  filter(state == "NY" & response == "Excellent") %>% 
  filter(year == 2002 | year == 2006 | year == 2010) %>%
  group_by(county) %>%
  summarize(mean_prop = mean(data_value, na.rm = TRUE),
            sd_prop = sd(data_value, na.rm = TRUE)) %>% 
  knitr::kable(digits = 3)
```

| county             |  mean\_prop|  sd\_prop|
|:-------------------|-----------:|---------:|
| Bronx County       |      17.600|       NaN|
| Erie County        |      17.200|       NaN|
| Kings County       |      20.367|     1.767|
| Monroe County      |      22.400|       NaN|
| Nassau County      |      24.933|     2.822|
| New York County    |      27.500|     1.539|
| Queens County      |      19.633|     1.358|
| Suffolk County     |      24.100|     3.279|
| Westchester County |      26.450|     0.636|

New York County had the highest mean proportion of "Excellent" response (27.500) in New York State in year 2002, 2006 and 2010, while Suffolk County have the largest standard deviation (3.279).
There was no standard deviation of Bronx County, Erie County and Monroe County because they had observations only in one of the three years.

Problem 1\_5 Five-panel plot showing the distribution of state-level averages over time
---------------------------------------------------------------------------------------

``` r
brfss %>% 
  group_by(year, state, response) %>% 
  summarize(mean_prop = mean(data_value, na.rm = TRUE)) %>% 
  ggplot(aes(x = year, y = mean_prop, color = state)) +
    geom_line() +
    facet_grid(. ~ response) +
    labs(
      title = "Distribution of State-level Response over time",
      x = "Year",
      y = "Mean Response Proportion"
      ) +
    theme(axis.text.x = element_text(hjust = 1, angle = 45))
```

<img src="p8105_hw3_st3117_files/figure-markdown_github/Problem 1_5-1.png" width="90%" />

Across five responses categories, "Very good" had the highest overall proportion over time, followed by "Good", "Excellent", "Fair" and "Poor". The distribution of these state-level averages are relatively stable over time.
The difference between states in "Very good" and "Excellent" are noticeably larger than the difference between states within "Poor" response.

Problem 2\_1 Instacart data cleaning and summarizing
----------------------------------------------------

``` r
instacart = instacart

# take a brief look at the data structure and variables

skimr::skim(instacart)
```

    ## Skim summary statistics
    ##  n obs: 1384617 
    ##  n variables: 15 
    ## 
    ## -- Variable type:character ----------------------------------------------------------------------------------------------
    ##      variable missing complete       n min max empty n_unique
    ##         aisle       0  1384617 1384617   3  29     0      134
    ##    department       0  1384617 1384617   4  15     0       21
    ##      eval_set       0  1384617 1384617   5   5     0        1
    ##  product_name       0  1384617 1384617   3 159     0    39123
    ## 
    ## -- Variable type:integer ------------------------------------------------------------------------------------------------
    ##                variable missing complete       n       mean        sd p0
    ##       add_to_cart_order       0  1384617 1384617       8.76      7.42  1
    ##                aisle_id       0  1384617 1384617      71.3      38.1   1
    ##  days_since_prior_order       0  1384617 1384617      17.07     10.43  0
    ##           department_id       0  1384617 1384617       9.84      6.29  1
    ##               order_dow       0  1384617 1384617       2.7       2.17  0
    ##       order_hour_of_day       0  1384617 1384617      13.58      4.24  0
    ##                order_id       0  1384617 1384617 1706297.62 989732.65  1
    ##            order_number       0  1384617 1384617      17.09     16.61  4
    ##              product_id       0  1384617 1384617   25556.24  14121.27  1
    ##               reordered       0  1384617 1384617       0.6       0.49  0
    ##                 user_id       0  1384617 1384617   1e+05     59487.15  1
    ##     p25     p50     p75    p100     hist
    ##       3       7      12      80 <U+2587><U+2583><U+2581><U+2581><U+2581><U+2581><U+2581><U+2581>
    ##      31      83     107     134 <U+2583><U+2587><U+2583><U+2583><U+2587><U+2585><U+2585><U+2586>
    ##       7      15      30      30 <U+2582><U+2585><U+2583><U+2583><U+2581><U+2582><U+2581><U+2587>
    ##       4       8      16      21 <U+2583><U+2587><U+2582><U+2581><U+2582><U+2586><U+2581><U+2583>
    ##       1       3       5       6 <U+2587><U+2585><U+2583><U+2583><U+2581><U+2583><U+2585><U+2585>
    ##      10      14      17      23 <U+2581><U+2581><U+2583><U+2587><U+2587><U+2587><U+2585><U+2582>
    ##  843370 1701880 2568023 3421070 <U+2587><U+2587><U+2587><U+2587><U+2587><U+2587><U+2587><U+2587>
    ##       6      11      21     100 <U+2587><U+2582><U+2581><U+2581><U+2581><U+2581><U+2581><U+2581>
    ##   13380   25298   37940   49688 <U+2586><U+2586><U+2587><U+2587><U+2587><U+2586><U+2587><U+2587>
    ##       0       1       1       1 <U+2586><U+2581><U+2581><U+2581><U+2581><U+2581><U+2581><U+2587>
    ##   51732   1e+05  154959  206209 <U+2587><U+2587><U+2587><U+2587><U+2587><U+2587><U+2587><U+2587>

``` r
# check distinct user IDs

count(distinct(instacart, user_id))
```

    ## # A tibble: 1 x 1
    ##        n
    ##    <int>
    ## 1 131209

``` r
# Find the most popular departments

instacart %>%
  group_by(department) %>%
  summarize(n_obs = n()) %>% 
  arrange(-n_obs) %>% 
  head(3)
```

    ## # A tibble: 3 x 2
    ##   department  n_obs
    ##   <chr>       <int>
    ## 1 produce    409087
    ## 2 dairy eggs 217051
    ## 3 snacks     118862

``` r
# Find the most popular products

instacart %>%
  group_by(product_name) %>%
  summarize(n_obs = n()) %>% 
  arrange(-n_obs) %>% 
  head(3)
```

    ## # A tibble: 3 x 2
    ##   product_name           n_obs
    ##   <chr>                  <int>
    ## 1 Banana                 18726
    ## 2 Bag of Organic Bananas 15480
    ## 3 Organic Strawberries   10894

``` r
# check the order day pattern

ggplot(instacart, aes(x = order_dow)) +
    geom_histogram() +
    labs(
      title = "Order Day",
      x = "Day of Week, where 0 = Sunday",
      y = "Count"
      )
```

<img src="p8105_hw3_st3117_files/figure-markdown_github/Problem 2_1-1.png" width="90%" />

``` r
# check the order time pattern

ggplot(instacart, aes(x = order_hour_of_day)) +
    geom_histogram() +
    labs(
      title = "Order Time",
      x = "Time",
      y = "Count"
      )
```

<img src="p8105_hw3_st3117_files/figure-markdown_github/Problem 2_1-2.png" width="90%" />

The dataset Instacart is a 15 by 1384617 table.
It contains 4 character variables and 11 numeric variables. Key variables can be user ID, departments, aisles and products if the research interest is about the order pattern. There is no missing data in this dataset.
There are 131209 distinct user IDs in total.
There are 21 departments, 134 aisles and 39123 unique products in the dataset. The 3 most popular departments are Produce, Dairy eggs and Snacks, while the 3 most popular products are Banana, Bag of Organic Bananas and Organic Strawberries.
From the histograms, we can see that the peak day of ordering happens at weekends, and the peak time of odering appears at noon and in the afternnon.</br>

Problem 2\_2 How many aisles are there, and which aisles are the most items ordered from?
-----------------------------------------------------------------------------------------

``` r
# cehck how many aisles there are

count(distinct(instacart, aisle_id)) 
```

    ## # A tibble: 1 x 1
    ##       n
    ##   <int>
    ## 1   134

``` r
# find which aisles are the most items ordered from

instacart %>%
  group_by(aisle) %>%
  summarize(n_obs = n()) %>% 
  arrange(-n_obs) %>% 
  head(3)
```

    ## # A tibble: 3 x 2
    ##   aisle                       n_obs
    ##   <chr>                       <int>
    ## 1 fresh vegetables           150609
    ## 2 fresh fruits               150473
    ## 3 packaged vegetables fruits  78493

There are 134 distinct aisles, and Fresh vegetables have the most items (150609) ordered from. The top 3 aisles are fresh vegetables, fresh fruits and packaged vegetables fruits, indicating people included in this dataset have a relatively healthy diet.

Problem 2\_3 Make a plot that shows the number of items ordered in each aisle
-----------------------------------------------------------------------------

``` r
instacart %>%
  group_by(aisle) %>%
  summarize(n_obs = n()) %>% 
  mutate(group = as.numeric(cut_number(n_obs, 3))) %>% 
  ggplot(aes(x = reorder(aisle, n_obs), y = n_obs)) +
    geom_point() +
    facet_wrap(group ~ ., nrow = 3, scales = "free") +
    theme(axis.text.x = element_text(size = 8, hjust = 1, angle = 45)) +
    labs(
      title = "Number of Items Ordered in Aisles",
      x = "Aisle Name",
      y = "Number of Items Ordered"
      )
```

<img src="p8105_hw3_st3117_files/figure-markdown_github/Problem 2_3-1.png" width="90%" />

``` r
# check the mean of number of items
instacart %>%
  group_by(aisle) %>%
  summarize(n_obs = n()) %>% 
  summarize(mean_n = mean(n_obs, na.rm = TRUE))
```

    ## # A tibble: 1 x 1
    ##   mean_n
    ##    <dbl>
    ## 1 10333.

Note: I devided the plot into three equal pieces so that the x-axis label can be seen.
The aisle with the smallest number of items ordered is beauty with 287 orders, while the aisle with the largest number of items ordered is fresh vegetables with 150609 orders. The mean of the number of items are 10333.
The number of items ordered from fresh vegetables and fresh fruits are significantly higher than the rest of the aisles.

Problem 2\_4 Make a table showing the most popular item in each of the aisles “baking ingredients”, “dog food care”, and “packaged vegetables fruits”
-----------------------------------------------------------------------------------------------------------------------------------------------------

``` r
instacart %>% 
  filter(aisle == "baking ingredients" | aisle == "dog food care" | aisle == "packaged vegetables fruits") %>% 
  group_by(aisle, product_name) %>% 
  summarize(n_obs = n()) %>% 
  arrange(aisle, -n_obs) %>% 
  filter(row_number() == 1) %>% 
  knitr::kable()
```

| aisle                      | product\_name                                 |  n\_obs|
|:---------------------------|:----------------------------------------------|-------:|
| baking ingredients         | Light Brown Sugar                             |     499|
| dog food care              | Snack Sticks Chicken & Rice Recipe Dog Treats |      30|
| packaged vegetables fruits | Organic Baby Spinach                          |    9784|

The most popular item in baking ingredients aisle is Light Brown Sugar with 499 times of order;
The most popular item in dog food care aisle is Snack Sticks Chicken & Rice Recipe Dog Treats with 30 times of order;  The most popular item in packaged vegetables fruits aisle is Organic Baby Spinach with 9784 times of order.
Between those three groups, we can see that people order packaged vegetables fruits much more than baking ingredients, and dog food care is seldomly ordered through the system.

Problem 2\_5 Make a table showing the mean hour of the day at which Pink Lady Apples and Coffee Ice Cream are ordered on each day of the week; format this table for human readers.
-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

``` r
instacart %>% 
  filter(product_name == "Pink Lady Apples" | product_name == "Coffee Ice Cream") %>% 
  group_by(product_name, order_dow) %>% 
  summarize(mean_hour = mean(order_hour_of_day, na.rm = TRUE)) %>% 
  spread(key = product_name, value = mean_hour) %>% 
  knitr::kable(digits = 2)
```

|  order\_dow|  Coffee Ice Cream|  Pink Lady Apples|
|-----------:|-----------------:|-----------------:|
|           0|             13.77|             13.44|
|           1|             14.32|             11.36|
|           2|             15.38|             11.70|
|           3|             15.32|             14.25|
|           4|             15.22|             11.55|
|           5|             12.26|             12.78|
|           6|             13.83|             11.94|

``` r
# check the group mean hour of mean hour of each day

instacart %>% 
  filter(product_name == "Pink Lady Apples" | product_name == "Coffee Ice Cream") %>% 
  group_by(product_name, order_dow) %>% 
  summarize(mean_hour = mean(order_hour_of_day, na.rm = TRUE)) %>% 
  summarize(mean_n = mean(mean_hour, na.rm = TRUE))
```

    ## # A tibble: 2 x 2
    ##   product_name     mean_n
    ##   <chr>             <dbl>
    ## 1 Coffee Ice Cream   14.3
    ## 2 Pink Lady Apples   12.4

From the table we can see that for Coffee Ice Cream, people tend to order it eariler on Friday (12.26), Saturday (13.83) and Sunday (13.77), and the mean hour is 14.3 for the group.
Fro Pink Lady Apples, people order it late on Sunday (13.44) and Wednesday (14.25), while the mean hour of order is 12.4 for the group.

Problem 3\_1
------------

``` r
ny_noaa = ny_noaa

# extract year, month and day from date, and do some unit convertion

ny_noaa = ny_noaa %>% 
            mutate(year = strftime(date, "%Y"),
                   month = strftime(date, "%B"),
                   day = strftime(date, "%d"),
                   prcp = prcp / 10,
                   snow = as.numeric(snow),
                   snwd = as.numeric(snwd),
                   tmax = as.numeric(tmax) / 10,
                   tmin = as.numeric(tmin) / 10) %>% 
            select(id, year, month, day, prcp_mm = prcp, snow_mm = snow, snwd_mm = snwd, tmax_c = tmax, 
                   tmin_c = tmin)

# briefly check the statistics

skimr::skim(ny_noaa)
```

    ## Skim summary statistics
    ##  n obs: 2595176 
    ##  n variables: 9 
    ## 
    ## -- Variable type:character ----------------------------------------------------------------------------------------------
    ##  variable missing complete       n min max empty n_unique
    ##       day       0  2595176 2595176   2   2     0       31
    ##        id       0  2595176 2595176  11  11     0      747
    ##     month       0  2595176 2595176   3   9     0       12
    ##      year       0  2595176 2595176   4   4     0       30
    ## 
    ## -- Variable type:numeric ------------------------------------------------------------------------------------------------
    ##  variable missing complete       n  mean     sd    p0  p25  p50  p75  p100
    ##   prcp_mm  145838  2449338 2595176  2.98   7.82   0    0    0    2.3  2286
    ##   snow_mm  381221  2213955 2595176  4.99  27.22 -13    0    0    0   10160
    ##   snwd_mm  591786  2003390 2595176 37.31 113.54   0    0    0    0    9195
    ##    tmax_c 1134358  1460818 2595176 13.98  11.14 -38.9  5   15   23.3    60
    ##    tmin_c 1134420  1460756 2595176  3.03  10.4  -59.4 -3.9  3.3 11.1    60
    ##      hist
    ##  <U+2587><U+2581><U+2581><U+2581><U+2581><U+2581><U+2581><U+2581>
    ##  <U+2587><U+2581><U+2581><U+2581><U+2581><U+2581><U+2581><U+2581>
    ##  <U+2587><U+2581><U+2581><U+2581><U+2581><U+2581><U+2581><U+2581>
    ##  <U+2581><U+2581><U+2582><U+2587><U+2587><U+2586><U+2581><U+2581>
    ##  <U+2581><U+2581><U+2581><U+2586><U+2587><U+2582><U+2581><U+2581>

``` r
# filter out the missing observations and check the statistics

ny_noaa_no_missing = ny_noaa %>% 
                       filter(!is.na(prcp_mm) & !is.na(snow_mm) & !is.na(snwd_mm) & !is.na(tmax_c) & !is.na(tmin_c))

skimr::skim(ny_noaa_no_missing)
```

    ## Skim summary statistics
    ##  n obs: 1222433 
    ##  n variables: 9 
    ## 
    ## -- Variable type:character ----------------------------------------------------------------------------------------------
    ##  variable missing complete       n min max empty n_unique
    ##       day       0  1222433 1222433   2   2     0       31
    ##        id       0  1222433 1222433  11  11     0      214
    ##     month       0  1222433 1222433   3   9     0       12
    ##      year       0  1222433 1222433   4   4     0       30
    ## 
    ## -- Variable type:numeric ------------------------------------------------------------------------------------------------
    ##  variable missing complete       n  mean     sd    p0  p25  p50  p75
    ##   prcp_mm       0  1222433 1222433  3.01   7.92   0    0    0    2.3
    ##   snow_mm       0  1222433 1222433  4.65  25.58 -13    0    0    0  
    ##   snwd_mm       0  1222433 1222433 36.66 112.72   0    0    0    0  
    ##    tmax_c       0  1222433 1222433 14.65  11    -38.9  5.6 16.1 23.9
    ##    tmin_c       0  1222433 1222433  3.63  10.25 -58.3 -2.8  4.4 11.7
    ##    p100     hist
    ##  2032.5 <U+2587><U+2581><U+2581><U+2581><U+2581><U+2581><U+2581><U+2581>
    ##  7765   <U+2587><U+2581><U+2581><U+2581><U+2581><U+2581><U+2581><U+2581>
    ##  9195   <U+2587><U+2581><U+2581><U+2581><U+2581><U+2581><U+2581><U+2581>
    ##    44.4 <U+2581><U+2581><U+2581><U+2583><U+2586><U+2587><U+2587><U+2581>
    ##    37.2 <U+2581><U+2581><U+2581><U+2582><U+2586><U+2587><U+2583><U+2581>

Note: I converted the unit of precipitation from tenths of mm to mm, and unit of temperatures from tenths of degrees C to degree C, which makes more sense.
The dataset ny\_noaa is a 9 by 2595176 table.
It contains 4 character variables and 5 numeric variables. Key variables can be year, month, prcp\_mm and snow\_mm if the research interest is about the precipitation and snow pattern in each months of the year.
Mean overall precipitation is 2.98 mm, mean snowfall is 4.99 mm, mean highest temperature is 13.98 celsius and the mean lowest temperature is 3.03 celsius.
There is around 5% of the missing data of prcp\_mm, 15% of snow\_mm, 23% of snwd\_mm, 44% of tmax\_c and tmin\_c.
As a rule of thumb, if less than 5% of the observations are missing, the missing data can simply be deleted without any significant ramifications. Althought the mean doesn't change much after we remove the observations with missing values, we have almost half of the temperature data missing, we'd better do some multiple imputation before analysis.

Problem 3\_2 For snowfall, what are the most commonly observed values?
----------------------------------------------------------------------

``` r
ny_noaa %>% 
  group_by(snow_mm) %>% 
  summarize(n_obs = n()) %>% 
  arrange(-n_obs) %>% 
  head(1)
```

    ## # A tibble: 1 x 2
    ##   snow_mm   n_obs
    ##     <dbl>   <int>
    ## 1       0 2008508

``` r
# check median for each month

ny_noaa %>% 
  group_by(month) %>% 
  summarize(median_snow = median(snow_mm, na.rm = TRUE))
```

    ## # A tibble: 12 x 2
    ##    month     median_snow
    ##    <chr>           <dbl>
    ##  1 April               0
    ##  2 August              0
    ##  3 December            0
    ##  4 February            0
    ##  5 January             0
    ##  6 July                0
    ##  7 June                0
    ##  8 March               0
    ##  9 May                 0
    ## 10 November            0
    ## 11 October             0
    ## 12 September           0

The most commonly observed values for snowfall is 0, with 2008508 observations. It makes sense because there's no snow for most of the month in New York. In addition, after checking the median of snowfall, we found that it is 0 even in winter.

Problem 3\_3 Make a two-panel plot showing the average max temperature in January and in July in each station across years.
---------------------------------------------------------------------------------------------------------------------------

``` r
ny_noaa %>% 
  filter(month == "January" | month == "July") %>% 
  group_by(id, year, month) %>% 
  summarize(mean_tmax = mean(tmax_c, na.rm = TRUE)) %>% 
  filter(!is.na(mean_tmax)) %>% 
  ggplot(aes(x = year, y = mean_tmax)) +
    geom_boxplot() +
    facet_grid(month ~ .) +
    labs(
      title = "Average Max Temperature in January and in July",
      x = "Month",
      y = "Average Max Temperature"
      ) +
    theme(axis.text.x = element_text(size = 8, hjust = 1, angle = 45))
```

<img src="p8105_hw3_st3117_files/figure-markdown_github/Problem 3_3-1.png" width="90%" />

``` r
ny_noaa %>% 
  filter(month == "January" | month == "July") %>% 
  group_by(id, year, month) %>% 
  summarize(mean_tmax = mean(tmax_c, na.rm = TRUE)) %>% 
  arrange(mean_tmax)
```

    ## # A tibble: 14,111 x 4
    ## # Groups:   id, year [7,409]
    ##    id          year  month   mean_tmax
    ##    <chr>       <chr> <chr>       <dbl>
    ##  1 USC00301723 1982  January    -13.4 
    ##  2 USC00305925 2005  January    -12.2 
    ##  3 USC00306957 2004  January    -10.8 
    ##  4 USC00304996 1994  January    -10.6 
    ##  5 USW00094725 2004  January    -10.4 
    ##  6 USW00094740 2004  January    -10.4 
    ##  7 USW00094725 1994  January    -10.2 
    ##  8 USC00304996 2004  January     -9.98
    ##  9 USC00306659 1994  January     -9.90
    ## 10 USC00301401 1994  January     -9.7 
    ## # ... with 14,101 more rows

``` r
ny_noaa %>% 
  filter(month == "January" | month == "July") %>% 
  group_by(id, year, month) %>% 
  summarize(mean_tmax = mean(tmax_c, na.rm = TRUE)) %>% 
  group_by(month, year) %>% 
  summarize(groupmean_tmax = mean(mean_tmax, na.rm = TRUE)) %>% 
  arrange(groupmean_tmax) %>% View
```

The higher max temperature in January will result a higher max temperature in July in most of the times across years.
There are two extreme outliers in January, one is -13.44 degree in 1982, and the other is -12.20 degree in 2005.
There is one extreme outlier in July, which is 13.95 degree in 1988.
Overall, the mean max temperature in January aross year is from -5 degree to 5 degree, with no obvious outlier; while the mean max temperature in July aross year is from 24 degree to 29 degree, with no obvious outlier.

Problem 3\_4 Make a plot showing tmax vs tmin for the full dataset
------------------------------------------------------------------

``` r
tmax_tmin_p = ggplot(ny_noaa, aes(x = tmax_c, y = tmin_c)) + 
                geom_hex() +
                labs(
                  title = "maximum temperature vs minimum temperature",
                  x = "maximum temperature celsius",
                  y = "minimum temperature celsius"
                  )
```

Problem 3\_5 make a plot showing the distribution of snowfall values greater than 0 and less than 100 separately by year
------------------------------------------------------------------------------------------------------------------------

``` r
snow_dist_p = ny_noaa %>% 
                filter(snow_mm > 0 & snow_mm < 100) %>% 
                ggplot(aes(x = snow_mm, y = year)) +
                  geom_density_ridges(scale = 0.85) +
                  labs(
                    title = "Distribution of Snowfall",
                    x = "Snowfall in mm",
                    y = "Year"
                    )
```

Problem 3\_6 Make the previous plots a two-panel plot
-----------------------------------------------------

``` r
tmax_tmin_p / snow_dist_p
```

    ## Warning: Removed 1136276 rows containing non-finite values (stat_binhex).

    ## Picking joint bandwidth of 3.76

<img src="p8105_hw3_st3117_files/figure-markdown_github/Problem 3_6-1.png" width="90%" />

The maximum temperatures are linearly associated with the minimum temperatures, which means the higher the maximum temperatures, the higher the minimum temperatures. The highest density occured from tmin = 0 with tmax = 8 to tmin = 15 with tmax = 28 with around 5000 counts.
The distribution of snowfall have almost the same patterns across years. The highest peak occured twice at 10 mm and 25 mm. There are also two peaks at around 45 mm and 75 mm.
