p8105\_hw2\_st3117
================
Sha Tao
October 6, 2018

Problem 1\_1 BRFSS data cleaning
--------------------------------

``` r
brfss = 
  brfss_smart2010 %>% 
  janitor::clean_names() %>%
  filter(topic == "Overall Health") %>% 
  separate(locationdesc, into = c("remove2", "county"), sep = 5) %>% 
  select(year, state = locationabbr, county, response, data_value) %>% 
  mutate(response = as.factor(response),
         response = factor(response, levels = c("Excellent", "Very good", "Good", "Fair", "Poor")))

# check the factor level of response

levels(brfss$response)
```

    ## [1] "Excellent" "Very good" "Good"      "Fair"      "Poor"

Problem 1\_2 In 2002, which states were observed at 7 locations?
----------------------------------------------------------------

``` r
brfss %>% 
  spread(key = response, value = data_value) %>% 
  janitor::clean_names() %>% 
  filter(year == 2002) %>% 
  group_by(state) %>% 
  summarize(n = n()) %>% 
  filter(n == 7)
```

    ## # A tibble: 3 x 2
    ##   state     n
    ##   <chr> <int>
    ## 1 CT        7
    ## 2 FL        7
    ## 3 NC        7

``` r
brfss %>% 
  filter(year == 2002) %>% 
  group_by(state) %>% 
  summarize(n = n_distinct(county)) %>% 
  filter(n == 7)
```

    ## # A tibble: 3 x 2
    ##   state     n
    ##   <chr> <int>
    ## 1 CT        7
    ## 2 FL        7
    ## 3 NC        7

Connecticut, Florida and North Carolina were observed at 7 locations in 2002.

Problem 1\_3 Spaghetti plot showing the number of observations in each state from 2002 to 2010s
-----------------------------------------------------------------------------------------------

``` r
brfss %>%
  group_by(year, state) %>% 
  summarize(n_obs = n()) %>% 
  ggplot(aes(x = year, y = n_obs, color = state)) +
    geom_line() +
    labs(
      title = "Number of Observations Plot",
      x = "Year",
      y = "Number of Observations"
      ) +
    theme(legend.position = "right")
```

![](p8105_hw3_st3117_files/figure-markdown_github/Problem%201_3-1.png)

``` r
brfss %>%
  group_by(year, state) %>% 
  summarize(n_obs = n()) %>% 
  arrange(-n_obs)
```

    ## # A tibble: 443 x 3
    ## # Groups:   year [9]
    ##     year state n_obs
    ##    <int> <chr> <int>
    ##  1  2007 FL      220
    ##  2  2010 FL      205
    ##  3  2005 NJ       95
    ##  4  2006 NJ       95
    ##  5  2009 NJ       95
    ##  6  2010 NJ       95
    ##  7  2008 NJ       90
    ##  8  2007 NJ       80
    ##  9  2008 NC       80
    ## 10  2010 TX       80
    ## # ... with 433 more rows

From the spaghetti plot, we can see most of the states had relatively stable number of ovservations across years. There were two peaks at 2007 and 2010 for Florida. Comparing with the table, we see that the observations in 2007 reached 220 and it reached 205 in 2010.

Problem 1\_4 NY State “Excellent” responses
-------------------------------------------

``` r
brfss %>%
  filter(state == "NY" & response == "Excellent") %>% 
  filter(year == 2002 | year == 2006 | year == 2010) %>%
  group_by(county) %>%
  summarize(mean_prop = mean(data_value, na.rm = TRUE),
            sd_prop = sd(data_value, na.rm = TRUE)) %>% 
  knitr::kable(digits = 3)
```

| county             |  mean\_prop|  sd\_prop|
|:-------------------|-----------:|---------:|
| Bronx County       |      17.600|       NaN|
| Erie County        |      17.200|       NaN|
| Kings County       |      20.367|     1.767|
| Monroe County      |      22.400|       NaN|
| Nassau County      |      24.933|     2.822|
| New York County    |      27.500|     1.539|
| Queens County      |      19.633|     1.358|
| Suffolk County     |      24.100|     3.279|
| Westchester County |      26.450|     0.636|

New York County had the highest mean proportion of "Excellent" response (27.500) in New York State in year 2002, 2006 and 2010, while Suffolk County have the largest standard deviation (3.279). There was no standard deviation of Bronx County, Erie County and Monroe County because they had observations only in one of the three years.

Problem 1\_5 Five-panel plot showing the distribution of state-level averages over time
---------------------------------------------------------------------------------------

``` r
brfss %>% 
  group_by(year, state, response) %>% 
  summarize(mean_prop = mean(data_value, na.rm = TRUE)) %>% 
  ggplot(aes(x = year, y = mean_prop, color = state)) +
    geom_line() +
    facet_grid(. ~ response) +
    labs(
      title = "Distribution of State-level Response over time",
      x = "Year",
      y = "Mean Response Proportion"
      ) +
    theme(axis.text.x = element_text(hjust = 1, angle = 45))
```

![](p8105_hw3_st3117_files/figure-markdown_github/Problem%201_5-1.png)

Across five responses categories, "Very good" had the highest overall proportion over time, followed by "Good", "Excellent", "Fair" and "Poor". The distribution of these state-level averages are relatively stable over time.

Problem 2\_1 Instacart data cleaning and summarizing
----------------------------------------------------

``` r
instacart = instacart

# take a brief look at the data structure and variables

skimr::skim(instacart)
```

    ## Skim summary statistics
    ##  n obs: 1384617 
    ##  n variables: 15 
    ## 
    ## -- Variable type:character ----------------------------------------------------------------------------------------------
    ##      variable missing complete       n min max empty n_unique
    ##         aisle       0  1384617 1384617   3  29     0      134
    ##    department       0  1384617 1384617   4  15     0       21
    ##      eval_set       0  1384617 1384617   5   5     0        1
    ##  product_name       0  1384617 1384617   3 159     0    39123
    ## 
    ## -- Variable type:integer ------------------------------------------------------------------------------------------------
    ##                variable missing complete       n       mean        sd p0
    ##       add_to_cart_order       0  1384617 1384617       8.76      7.42  1
    ##                aisle_id       0  1384617 1384617      71.3      38.1   1
    ##  days_since_prior_order       0  1384617 1384617      17.07     10.43  0
    ##           department_id       0  1384617 1384617       9.84      6.29  1
    ##               order_dow       0  1384617 1384617       2.7       2.17  0
    ##       order_hour_of_day       0  1384617 1384617      13.58      4.24  0
    ##                order_id       0  1384617 1384617 1706297.62 989732.65  1
    ##            order_number       0  1384617 1384617      17.09     16.61  4
    ##              product_id       0  1384617 1384617   25556.24  14121.27  1
    ##               reordered       0  1384617 1384617       0.6       0.49  0
    ##                 user_id       0  1384617 1384617   1e+05     59487.15  1
    ##     p25     p50     p75    p100     hist
    ##       3       7      12      80 <U+2587><U+2583><U+2581><U+2581><U+2581><U+2581><U+2581><U+2581>
    ##      31      83     107     134 <U+2583><U+2587><U+2583><U+2583><U+2587><U+2585><U+2585><U+2586>
    ##       7      15      30      30 <U+2582><U+2585><U+2583><U+2583><U+2581><U+2582><U+2581><U+2587>
    ##       4       8      16      21 <U+2583><U+2587><U+2582><U+2581><U+2582><U+2586><U+2581><U+2583>
    ##       1       3       5       6 <U+2587><U+2585><U+2583><U+2583><U+2581><U+2583><U+2585><U+2585>
    ##      10      14      17      23 <U+2581><U+2581><U+2583><U+2587><U+2587><U+2587><U+2585><U+2582>
    ##  843370 1701880 2568023 3421070 <U+2587><U+2587><U+2587><U+2587><U+2587><U+2587><U+2587><U+2587>
    ##       6      11      21     100 <U+2587><U+2582><U+2581><U+2581><U+2581><U+2581><U+2581><U+2581>
    ##   13380   25298   37940   49688 <U+2586><U+2586><U+2587><U+2587><U+2587><U+2586><U+2587><U+2587>
    ##       0       1       1       1 <U+2586><U+2581><U+2581><U+2581><U+2581><U+2581><U+2581><U+2587>
    ##   51732   1e+05  154959  206209 <U+2587><U+2587><U+2587><U+2587><U+2587><U+2587><U+2587><U+2587>

``` r
# check distinct user IDs

count(distinct(instacart, user_id))
```

    ## # A tibble: 1 x 1
    ##        n
    ##    <int>
    ## 1 131209

``` r
# Find the most popular departments

instacart %>%
  group_by(department) %>%
  summarize(n_obs = n()) %>% 
  arrange(-n_obs) %>% 
  head(3)
```

    ## # A tibble: 3 x 2
    ##   department  n_obs
    ##   <chr>       <int>
    ## 1 produce    409087
    ## 2 dairy eggs 217051
    ## 3 snacks     118862

``` r
# Find the most popular products

instacart %>%
  group_by(product_name) %>%
  summarize(n_obs = n()) %>% 
  arrange(-n_obs) %>% 
  head(3)
```

    ## # A tibble: 3 x 2
    ##   product_name           n_obs
    ##   <chr>                  <int>
    ## 1 Banana                 18726
    ## 2 Bag of Organic Bananas 15480
    ## 3 Organic Strawberries   10894

``` r
# check the order day pattern

ggplot(instacart, aes(x = order_dow)) +
    geom_histogram() +
    labs(
      title = "Order Day",
      x = "Day",
      y = "Count"
      )
```

    ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.

![](p8105_hw3_st3117_files/figure-markdown_github/Problem%202_1-1.png)

``` r
# check the order time pattern

ggplot(instacart, aes(x = order_hour_of_day)) +
    geom_histogram() +
    labs(
      title = "Order Time",
      x = "Time",
      y = "Count"
      )
```

    ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.

![](p8105_hw3_st3117_files/figure-markdown_github/Problem%202_1-2.png)

The dataset Instacart is a 15 by 1384617 table.
It contains 4 character variables and 11 numeric variables, and there is no missing data in this dataset.
There are 131209 distinct user IDs in total.
There are 21 departments, 134 aisles and 39123 unique products in the dataset. The 3 most popular departments are Produce, Dairy eggs and Snacks, while the 3 most popular products are Banana, Bag of Organic Bananas and Organic Strawberries.
From the histograms, we can see that the peak day of ordering happens at weekends, and the peak time of odering appears at noon and in the afternnon.</br>

Problem 2\_2 How many aisles are there, and which aisles are the most items ordered from?
-----------------------------------------------------------------------------------------

``` r
## How many aisles are there, and which aisles are the most items ordered from?

count(distinct(instacart, aisle_id)) 
```

    ## # A tibble: 1 x 1
    ##       n
    ##   <int>
    ## 1   134

``` r
instacart %>%
  summarize(n = n_distinct(aisle_id))
```

    ## # A tibble: 1 x 1
    ##       n
    ##   <int>
    ## 1   134

``` r
instacart %>%
  group_by(aisle) %>%
  summarize(n_obs = n()) %>% 
  arrange(-n_obs) %>% 
  head(3)
```

    ## # A tibble: 3 x 2
    ##   aisle                       n_obs
    ##   <chr>                       <int>
    ## 1 fresh vegetables           150609
    ## 2 fresh fruits               150473
    ## 3 packaged vegetables fruits  78493

There are 134 distinct aisles, and Fresh vegetables have the most items (150609) ordered from. The top 3 aisles are fresh vegetables, fresh fruits and packaged vegetables fruits, indicating people included in this dataset have a relatively healthy diet.

Problem 2\_3 Make a plot that shows the number of items ordered in each aisle
-----------------------------------------------------------------------------

``` r
instacart %>%
  group_by(aisle) %>%
  summarize(n_obs = n()) %>% 
  mutate(group = as.numeric(cut_number(n_obs, 3))) %>% 
  ggplot(aes(x = reorder(aisle, n_obs), y = n_obs)) +
    geom_point() +
    facet_wrap(group ~ ., nrow = 3, scales = "free") +
    theme(axis.text.x = element_text(size = 8, hjust = 1, angle = 45)) +
    labs(
      title = "Number of Items Ordered in Aisles",
      x = "Aisle Name",
      y = "Number of Items Ordered"
      )
```

![](p8105_hw3_st3117_files/figure-markdown_github/Problem%202_3-1.png)

``` r
instacart %>%
  group_by(aisle) %>%
  summarize(n_obs = n()) %>% 
  arrange(n_obs) %>% 
  View
```

The aisle with the smallest number of items ordered is beauty with 287 orders, while the aisle with the largest number of items ordered is fresh vegetables with 150609 orders.

Problem 2\_4 Make a table showing the most popular item in each of the aisles “baking ingredients”, “dog food care”, and “packaged vegetables fruits”
-----------------------------------------------------------------------------------------------------------------------------------------------------

``` r
instacart %>% 
  filter(aisle == "baking ingredients" | aisle == "dog food care" | aisle == "packaged vegetables fruits") %>% 
  group_by(aisle, product_name) %>% 
  summarize(n_obs = n()) %>% 
  arrange(aisle, -n_obs) %>% 
  filter(row_number() == 1) %>% 
  knitr::kable()
```

| aisle                      | product\_name                                 |  n\_obs|
|:---------------------------|:----------------------------------------------|-------:|
| baking ingredients         | Light Brown Sugar                             |     499|
| dog food care              | Snack Sticks Chicken & Rice Recipe Dog Treats |      30|
| packaged vegetables fruits | Organic Baby Spinach                          |    9784|

Problem 2\_5 Make a table showing the mean hour of the day at which Pink Lady Apples and Coffee Ice Cream are ordered on each day of the week; format this table for human readers (i.e. produce a 2 x 7 table)
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

``` r
instacart %>% 
  filter(product_name == "Pink Lady Apples" | product_name == "Coffee Ice Cream") %>% 
  group_by(product_name, order_dow) %>% 
  summarize(mean_hour = mean(order_hour_of_day, na.rm = TRUE)) %>% 
  spread(key = product_name, value = mean_hour) %>% 
  knitr::kable(digits = 2)
```

|  order\_dow|  Coffee Ice Cream|  Pink Lady Apples|
|-----------:|-----------------:|-----------------:|
|           0|             13.77|             13.44|
|           1|             14.32|             11.36|
|           2|             15.38|             11.70|
|           3|             15.32|             14.25|
|           4|             15.22|             11.55|
|           5|             12.26|             12.78|
|           6|             13.83|             11.94|

Problem 3\_1
------------

``` r
ny_noaa = ny_noaa

skimr::skim(ny_noaa)
```

    ## Skim summary statistics
    ##  n obs: 2595176 
    ##  n variables: 7 
    ## 
    ## -- Variable type:character ----------------------------------------------------------------------------------------------
    ##  variable missing complete       n min max empty n_unique
    ##        id       0  2595176 2595176  11  11     0      747
    ##      tmax 1134358  1460818 2595176   1   4     0      532
    ##      tmin 1134420  1460756 2595176   1   4     0      548
    ## 
    ## -- Variable type:Date ---------------------------------------------------------------------------------------------------
    ##  variable missing complete       n        min        max     median
    ##      date       0  2595176 2595176 1981-01-01 2010-12-31 1997-01-21
    ##  n_unique
    ##     10957
    ## 
    ## -- Variable type:integer ------------------------------------------------------------------------------------------------
    ##  variable missing complete       n  mean     sd  p0 p25 p50 p75  p100
    ##      prcp  145838  2449338 2595176 29.82  78.18   0   0   0  23 22860
    ##      snow  381221  2213955 2595176  4.99  27.22 -13   0   0   0 10160
    ##      snwd  591786  2003390 2595176 37.31 113.54   0   0   0   0  9195
    ##      hist
    ##  <U+2587><U+2581><U+2581><U+2581><U+2581><U+2581><U+2581><U+2581>
    ##  <U+2587><U+2581><U+2581><U+2581><U+2581><U+2581><U+2581><U+2581>
    ##  <U+2587><U+2581><U+2581><U+2581><U+2581><U+2581><U+2581><U+2581>

``` r
ny_noaa = ny_noaa %>% 
  mutate(year = strftime(date, "%Y"),
         month = strftime(date, "%B"),
         day = strftime(date, "%d"),
         prcp = prcp / 10,
         snow = as.numeric(snow),
         snwd = as.numeric(snwd),
         tmax = as.numeric(tmax) / 10,
         tmin = as.numeric(tmin) / 10) %>% 
  select(id, year, month, day, prcp_mm = prcp, snow_mm = snow, snwd_mm = snwd, tmax_c = tmax, tmin_c = tmin)


ny_noaa %>% 
  filter()
```

    ## # A tibble: 2,595,176 x 9
    ##    id          year  month    day   prcp_mm snow_mm snwd_mm tmax_c tmin_c
    ##    <chr>       <chr> <chr>    <chr>   <dbl>   <dbl>   <dbl>  <dbl>  <dbl>
    ##  1 US1NYAB0001 2007  November 01         NA      NA      NA     NA     NA
    ##  2 US1NYAB0001 2007  November 02         NA      NA      NA     NA     NA
    ##  3 US1NYAB0001 2007  November 03         NA      NA      NA     NA     NA
    ##  4 US1NYAB0001 2007  November 04         NA      NA      NA     NA     NA
    ##  5 US1NYAB0001 2007  November 05         NA      NA      NA     NA     NA
    ##  6 US1NYAB0001 2007  November 06         NA      NA      NA     NA     NA
    ##  7 US1NYAB0001 2007  November 07         NA      NA      NA     NA     NA
    ##  8 US1NYAB0001 2007  November 08         NA      NA      NA     NA     NA
    ##  9 US1NYAB0001 2007  November 09         NA      NA      NA     NA     NA
    ## 10 US1NYAB0001 2007  November 10         NA      NA      NA     NA     NA
    ## # ... with 2,595,166 more rows

Problem 3\_2
------------

``` r
ny_noaa %>% 
  group_by(snow_mm) %>% 
  summarize(n_obs = n()) %>% 
  arrange(-n_obs) %>% 
  head(1)
```

    ## # A tibble: 1 x 2
    ##   snow_mm   n_obs
    ##     <dbl>   <int>
    ## 1       0 2008508

``` r
ny_noaa %>% 
  group_by(month) %>% 
  summarize(mean_snow = median(snow_mm, na.rm = TRUE))
```

    ## # A tibble: 12 x 2
    ##    month     mean_snow
    ##    <chr>         <dbl>
    ##  1 April             0
    ##  2 August            0
    ##  3 December          0
    ##  4 February          0
    ##  5 January           0
    ##  6 July              0
    ##  7 June              0
    ##  8 March             0
    ##  9 May               0
    ## 10 November          0
    ## 11 October           0
    ## 12 September         0

Problem 3\_3 Make a two-panel plot showing the average max temperature in January and in July in each station across years. Is there any observable / interpretable structure? Any outliers?
--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

``` r
ny_noaa %>% 
  filter(month == "January" | month == "July") %>% 
  group_by(id, year, month) %>% 
  summarize(mean_tmax = mean(tmax_c, na.rm = TRUE)) %>% 
  filter(!is.na(mean_tmax)) %>% 
  ggplot(aes(x = year, y = mean_tmax)) +
    geom_boxplot() +
    facet_grid(month ~ .) +
    labs(
      title = "Average Max Temperature in January and in July",
      x = "Month",
      y = "Average Max Temperature"
      ) +
    theme(axis.text.x = element_text(size = 8, hjust = 1, angle = 45))
```

<img src="p8105_hw3_st3117_files/figure-markdown_github/Problem 3_3-1.png" width="800px" height="800px" />

``` r
is_outlier = function(x) {
  return(x < quantile(x, 0.25) - 1.5 * IQR(x) | x > quantile(x, 0.75) + 1.5 * IQR(x))
}

ny_noaa %>% 
  filter(month == "January" | month == "July") %>% 
  group_by(id, month) %>% 
  summarize(mean_tmax = mean(tmax_c, na.rm = TRUE)) %>% 
  filter(!is.na(mean_tmax)) %>% 
  group_by(month) %>% 
  mutate(outlier = ifelse(is_outlier(mean_tmax), mean_tmax, as.numeric(NA))) %>% 
  filter(!is.na(outlier)) %>% 
  arrange(month)
```

    ## # A tibble: 10 x 4
    ## # Groups:   month [2]
    ##    id          month   mean_tmax outlier
    ##    <chr>       <chr>       <dbl>   <dbl>
    ##  1 USC00301723 January     -9.98   -9.98
    ##  2 USC00302720 January     -8.02   -8.02
    ##  3 USC00305380 January      5.67    5.67
    ##  4 USC00305925 January    -12.2   -12.2 
    ##  5 USC00300424 July        23.5    23.5 
    ##  6 USC00304102 July        23.5    23.5 
    ##  7 USC00305769 July        23.3    23.3 
    ##  8 USC00305804 July        31.5    31.5 
    ##  9 USC00307799 July        22.4    22.4 
    ## 10 USC00308248 July        23.5    23.5

Problem 3\_4 Make a two-panel plot showing (i) tmax vs tmin for the full dataset (note that a scatterplot may not be the best option); (ii) make a plot showing the distribution of snowfall values greater than 0 and less than 100 separately by year.
--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

``` r
tmax_tmin_p = ggplot(ny_noaa, aes(x = tmax_c, y = tmin_c)) + 
                geom_hex() +
                labs(
                  title = "tmax vs tmin",
                  x = "tmax",
                  y = "tmin"
                  )
```

Problem 3\_5 and
----------------

``` r
snow_dist_p = ny_noaa %>% 
                filter(snow_mm > 0 & snow_mm < 100) %>% 
                ggplot(aes(x = year, y = snow_mm)) +
                  geom_violin() +
                  theme(axis.text.x = element_text(size = 8, hjust = 1, angle = 45)) +
                  labs(
                    title = "Distribution of Snowfall",
                    x = "Year",
                    y = "Snowfall in mm"
                    )



snow_dist_p = ny_noaa %>% 
                filter(snow_mm > 0 & snow_mm < 100) %>% 
                ggplot(aes(x = snow_mm, y = year)) +
                  geom_density_ridges(scale = 0.85) +
                  labs(
                    title = "Distribution of Snowfall",
                    x = "Snowfall in mm",
                    y = "Year"
                    )
```

``` r
tmax_tmin_p / snow_dist_p
```

    ## Warning: Removed 1136276 rows containing non-finite values (stat_binhex).

    ## Picking joint bandwidth of 3.76

![](p8105_hw3_st3117_files/figure-markdown_github/unnamed-chunk-1-1.png)
