---
title: "p8105_hw2_st3117"
author: "Sha Tao"
date: "October 6, 2018"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(patchwork)
library(hexbin)
library(ggridges)
library(p8105.datasets)

knitr::opts_chunk$set(out.width = "90%")

theme_set(theme_bw())
```

## Problem 1_1 BRFSS data cleaning

```{r Problem 1_1}

brfss = 
  brfss_smart2010 %>% 
  janitor::clean_names() %>%
  filter(topic == "Overall Health") %>% 
  separate(locationdesc, into = c("remove", "county"), sep = 5) %>% 
  select(year, state = locationabbr, county, response, data_value, everything(), -remove) %>% 
  mutate(response = as.factor(response),
         response = factor(response, levels = c("Excellent", "Very good", "Good", "Fair", "Poor")))

# check the factor level of response

levels(brfss$response) %>% 
  knitr::kable()

```

## Problem 1_2 In 2002, which states were observed at 7 locations?

```{r Problem 1_2}

# check number of locations distribution in 2002

brfss %>% 
  filter(year == 2002) %>% 
  group_by(state) %>% 
  summarize(n = n_distinct(county)) %>% 
  ggplot(aes(x = reorder(state, n), y = n)) +
    geom_point() +
    labs(
      title = "Number of Locations Plot",
      x = "States",
      y = "Number of Locationss"
      ) +
    theme(axis.text.x = element_text(size = 7, hjust = 1, angle = 45))

# check mean of the number of locations distribution in 2002

brfss %>% 
  filter(year == 2002) %>% 
  group_by(state) %>% 
  summarize(n = n_distinct(county)) %>% 
  summarize(mean_n = mean(n, na.rm = TRUE))

# find states which were observed at 7 locations in 2002

brfss %>% 
  filter(year == 2002) %>% 
  group_by(state) %>% 
  summarize(n = n_distinct(county)) %>% 
  filter(n == 7) %>% 
  knitr::kable()

```

Connecticut, Florida and North Carolina were observed at 7 locations in 2002.\
Among all the states, they ranked 4th from the highest observations which are Pennsylvania (10).\
The mean number of locations are 3.2.

## Problem 1_3 Spaghetti plot showing the number of observations in each state from 2002 to 2010s

```{r Problem 1_3}

# spaghetti plot

brfss %>%
  group_by(year, state) %>% 
  summarize(n = n_distinct(county)) %>% 
  ggplot(aes(x = year, y = n, color = state)) +
    geom_line() +
    labs(
      title = "Number of Observations Plot",
      x = "Year",
      y = "Number of Observations"
      ) +
    theme(legend.position = "right")

# check the outliers

brfss %>%
  group_by(year, state) %>% 
  summarize(n = n_distinct(county)) %>% 
  arrange(-n) %>% 
  head(3) %>% 
  knitr::kable()

```

From the spaghetti plot, we can see most of the states had relatively stable number of ovservations across years, from 0 to 20. There were two peaks at 2007 and 2010 for Florida. Comparing with the table, we see that the observations in 2007 reached 44 and it reached 41 in 2010.

## Problem 1_4 NY State “Excellent” responses

```{r Problem 1_4}

brfss %>%
  filter(state == "NY" & response == "Excellent") %>% 
  filter(year == 2002 | year == 2006 | year == 2010) %>%
  group_by(county) %>%
  summarize(mean_prop = mean(data_value, na.rm = TRUE),
            sd_prop = sd(data_value, na.rm = TRUE)) %>% 
  knitr::kable(digits = 3)

```

New York County had the highest mean proportion of "Excellent" response (27.500) in New York State in year 2002, 2006 and 2010, while Suffolk County have the largest standard deviation (3.279).\
There was no standard deviation of Bronx County, Erie County and Monroe County because they had observations only in one of the three years.

## Problem 1_5  Five-panel plot showing the distribution of state-level averages over time

```{r Problem 1_5}

brfss %>% 
  group_by(year, state, response) %>% 
  summarize(mean_prop = mean(data_value, na.rm = TRUE)) %>% 
  ggplot(aes(x = year, y = mean_prop, color = state)) +
    geom_line() +
    facet_grid(. ~ response) +
    labs(
      title = "Distribution of State-level Response over time",
      x = "Year",
      y = "Mean Response Proportion"
      ) +
    theme(axis.text.x = element_text(hjust = 1, angle = 45))

```

Across five responses categories, "Very good" had the highest overall proportion over time, followed by "Good", "Excellent", "Fair" and "Poor". The distribution of these state-level averages are relatively stable over time.\
The difference between states in "Very good" and "Excellent" are noticeably larger than the difference between states within "Poor" response.

## Problem 2_1 Instacart data cleaning and summarizing

```{r Problem 2_1, message = FALSE}

instacart = instacart

# take a brief look at the data structure and variables

skimr::skim(instacart)

# check distinct user IDs

count(distinct(instacart, user_id))

# Find the most popular departments

instacart %>%
  group_by(department) %>%
  summarize(n_obs = n()) %>% 
  arrange(-n_obs) %>% 
  head(3)

# Find the most popular products

instacart %>%
  group_by(product_name) %>%
  summarize(n_obs = n()) %>% 
  arrange(-n_obs) %>% 
  head(3)

# check the order day pattern

ggplot(instacart, aes(x = order_dow)) +
    geom_histogram() +
    labs(
      title = "Order Day",
      x = "Day of Week, where 0 = Sunday",
      y = "Count"
      )

# check the order time pattern

ggplot(instacart, aes(x = order_hour_of_day)) +
    geom_histogram() +
    labs(
      title = "Order Time",
      x = "Time",
      y = "Count"
      )

```

The dataset Instacart is a `r ncol(instacart)` by `r nrow(instacart)` table.\
It contains 4 character variables and 11 numeric variables. Key variables can be user ID, departments, aisles and products if the research interest is about the order pattern. There is no missing data in this dataset.\
There are 131209 distinct user IDs in total.\
There are 21 departments, 134 aisles and 39123 unique products in the dataset. The 3 most popular departments are Produce, Dairy eggs and Snacks, while the 3 most popular products are Banana, Bag of Organic Bananas and Organic Strawberries.\
From the histograms, we can see that the peak day of ordering happens at weekends, and the peak time of odering appears at noon and in the afternnon.</br>

## Problem 2_2  How many aisles are there, and which aisles are the most items ordered from?

```{r Problem 2_2}

# cehck how many aisles there are

count(distinct(instacart, aisle_id)) 

# find which aisles are the most items ordered from

instacart %>%
  group_by(aisle) %>%
  summarize(n_obs = n()) %>% 
  arrange(-n_obs) %>% 
  head(3)

```

There are 134 distinct aisles, and Fresh vegetables have the most items (150609) ordered from. The top 3 aisles are fresh vegetables, fresh fruits and packaged vegetables fruits, indicating people included in this dataset have a relatively healthy diet.

## Problem 2_3 Make a plot that shows the number of items ordered in each aisle

```{r Problem 2_3, fig.width = 10, fig.height = 18}

instacart %>%
  group_by(aisle) %>%
  summarize(n_obs = n()) %>% 
  mutate(group = as.numeric(cut_number(n_obs, 3))) %>% 
  ggplot(aes(x = reorder(aisle, n_obs), y = n_obs)) +
    geom_point() +
    facet_wrap(group ~ ., nrow = 3, scales = "free") +
    theme(axis.text.x = element_text(size = 8, hjust = 1, angle = 45)) +
    labs(
      title = "Number of Items Ordered in Aisles",
      x = "Aisle Name",
      y = "Number of Items Ordered"
      )

# check the mean of number of items
instacart %>%
  group_by(aisle) %>%
  summarize(n_obs = n()) %>% 
  summarize(mean_n = mean(n_obs, na.rm = TRUE))

```

Note: I devided the plot into three equal pieces so that the x-axis label can be seen.\
The aisle with the smallest number of items ordered is beauty with 287 orders, while the aisle with the largest number of items ordered is fresh vegetables with 150609 orders. The mean of the number of items are 10333.\
The number of items ordered from fresh vegetables and fresh fruits are significantly higher than the rest of the aisles.

## Problem 2_4 Make a table showing the most popular item in each of the aisles “baking ingredients”, “dog food care”, and “packaged vegetables fruits”

```{r Problem 2_4}

instacart %>% 
  filter(aisle == "baking ingredients" | aisle == "dog food care" | aisle == "packaged vegetables fruits") %>% 
  group_by(aisle, product_name) %>% 
  summarize(n_obs = n()) %>% 
  arrange(aisle, -n_obs) %>% 
  filter(row_number() == 1) %>% 
  knitr::kable()

```

The most popular item in baking ingredients aisle is Light Brown Sugar with 499 times of order;\
The most popular item in dog food care aisle is Snack Sticks Chicken & Rice Recipe Dog Treats with 30 times of order;\ 
The most popular item in packaged vegetables fruits aisle is Organic Baby Spinach with 9784 times of order.\
Between those three groups, we can see that people order packaged vegetables fruits much more than baking ingredients, and dog food care is seldomly ordered through the system.

## Problem 2_5 Make a table showing the mean hour of the day at which Pink Lady Apples and Coffee Ice Cream are ordered on each day of the week; format this table for human readers.

```{r Problem 2_5}

instacart %>% 
  filter(product_name == "Pink Lady Apples" | product_name == "Coffee Ice Cream") %>% 
  group_by(product_name, order_dow) %>% 
  summarize(mean_hour = mean(order_hour_of_day, na.rm = TRUE)) %>% 
  spread(key = product_name, value = mean_hour) %>% 
  knitr::kable(digits = 2)

# check the group mean hour of mean hour of each day

instacart %>% 
  filter(product_name == "Pink Lady Apples" | product_name == "Coffee Ice Cream") %>% 
  group_by(product_name, order_dow) %>% 
  summarize(mean_hour = mean(order_hour_of_day, na.rm = TRUE)) %>% 
  summarize(mean_n = mean(mean_hour, na.rm = TRUE))
  
```

From the table we can see that for Coffee Ice Cream, people tend to order it eariler on Friday (12.26), Saturday (13.83) and Sunday (13.77), and the mean hour is 14.3 for the group.\
Fro Pink Lady Apples, people order it late on Sunday (13.44) and Wednesday (14.25), while the mean hour of order is 12.4 for the group.

## Problem 3_1

```{r Problem 3_1}

ny_noaa = ny_noaa

# extract year, month and day from date, and do some unit convertion

ny_noaa = ny_noaa %>% 
            mutate(year = strftime(date, "%Y"),
                   month = strftime(date, "%B"),
                   day = strftime(date, "%d"),
                   prcp = prcp / 10,
                   snow = as.numeric(snow),
                   snwd = as.numeric(snwd),
                   tmax = as.numeric(tmax) / 10,
                   tmin = as.numeric(tmin) / 10) %>% 
            select(id, year, month, day, prcp_mm = prcp, snow_mm = snow, snwd_mm = snwd, tmax_c = tmax, 
                   tmin_c = tmin)

# briefly check the statistics

skimr::skim(ny_noaa)

# filter out the missing observations and check the statistics

ny_noaa_no_missing = ny_noaa %>% 
                       filter(!is.na(prcp_mm) & !is.na(snow_mm) & !is.na(snwd_mm) & !is.na(tmax_c) & !is.na(tmin_c))

skimr::skim(ny_noaa_no_missing)

```

Note: I converted the unit of precipitation from tenths of mm to mm, and unit of temperatures from tenths of degrees C to degree C, which makes more sense.\
The dataset ny_noaa is a `r ncol(ny_noaa)` by `r nrow(ny_noaa)` table.\
It contains 4 character variables and 5 numeric variables. Key variables can be year, month, prcp_mm and snow_mm if  the research interest is about the precipitation and snow pattern in each months of the year.\
Mean overall precipitation is 2.98 mm, mean snowfall is 4.99 mm, mean highest temperature is 13.98 celsius and the mean lowest temperature is 3.03 celsius.\
There is around 5% of the missing data of prcp_mm, 15% of snow_mm, 23% of snwd_mm, 44% of tmax_c and tmin_c.\
As a rule of thumb, if less than 5% of the observations are missing, the missing data can simply be deleted without any significant ramifications. Althought the mean doesn't change much after we remove the observations with missing values, we have almost half of the temperature data missing, we'd better do some multiple imputation before analysis.

## Problem 3_2 For snowfall, what are the most commonly observed values?

```{r Problem 3_2}

ny_noaa %>% 
  group_by(snow_mm) %>% 
  summarize(n_obs = n()) %>% 
  arrange(-n_obs) %>% 
  head(1)

# check median for each month

ny_noaa %>% 
  group_by(month) %>% 
  summarize(median_snow = median(snow_mm, na.rm = TRUE))

```

The most commonly observed values for snowfall is 0, with 2008508 observations. It makes sense because there's no snow for most of the month in New York. In addition, after checking the median of snowfall, we found that it is 0 even in winter.

## Problem 3_3 Make a two-panel plot showing the average max temperature in January and in July in each station across years.

```{r Problem 3_3}

ny_noaa %>% 
  filter(month == "January" | month == "July") %>% 
  group_by(id, year, month) %>% 
  summarize(mean_tmax = mean(tmax_c, na.rm = TRUE)) %>% 
  filter(!is.na(mean_tmax)) %>% 
  ggplot(aes(x = year, y = mean_tmax)) +
    geom_boxplot() +
    facet_grid(month ~ .) +
    labs(
      title = "Average Max Temperature in January and in July",
      x = "Month",
      y = "Average Max Temperature"
      ) +
    theme(axis.text.x = element_text(size = 8, hjust = 1, angle = 45))

ny_noaa %>% 
  filter(month == "January" | month == "July") %>% 
  group_by(id, year, month) %>% 
  summarize(mean_tmax = mean(tmax_c, na.rm = TRUE)) %>% 
  arrange(mean_tmax)

ny_noaa %>% 
  filter(month == "January" | month == "July") %>% 
  group_by(id, year, month) %>% 
  summarize(mean_tmax = mean(tmax_c, na.rm = TRUE)) %>% 
  group_by(month, year) %>% 
  summarize(groupmean_tmax = mean(mean_tmax, na.rm = TRUE)) %>% 
  arrange(groupmean_tmax)

```

The higher max temperature in January will result a higher max temperature in July in most of the times across years.\
There are two extreme outliers in January, one is -13.44 degree in 1982, and the other is -12.20 degree in 2005.\
There is one extreme outlier in July, which is 13.95 degree in 1988.\
Overall, the mean max temperature in January aross year is from -5 degree to 5 degree, with no obvious outlier; while the mean max temperature in July aross year is from 24 degree to 29 degree, with no obvious outlier.

## Problem 3_4 Make a plot showing tmax vs tmin for the full dataset

```{r Problem 3_4}

tmax_tmin_p = ggplot(ny_noaa, aes(x = tmax_c, y = tmin_c)) + 
                geom_hex() +
                labs(
                  title = "maximum temperature vs minimum temperature",
                  x = "maximum temperature celsius",
                  y = "minimum temperature celsius"
                  )

```

## Problem 3_5 make a plot showing the distribution of snowfall values greater than 0 and less than 100 separately by year

```{r Problem 3_5}

snow_dist_p = ny_noaa %>% 
                filter(snow_mm > 0 & snow_mm < 100) %>% 
                ggplot(aes(x = snow_mm, y = year)) +
                  geom_density_ridges(scale = 0.85) +
                  labs(
                    title = "Distribution of Snowfall",
                    x = "Snowfall in mm",
                    y = "Year"
                    )

```

## Problem 3_6 Make the previous plots a two-panel plot

```{r Problem 3_6, fig.width = 10, fig.height = 16}

tmax_tmin_p / snow_dist_p

```

The maximum temperatures are linearly associated with the minimum temperatures, which means the higher the maximum temperatures, the higher the minimum temperatures. The highest density occured from tmin = 0 with tmax = 8 to tmin = 15 with tmax = 28 with around 5000 counts.\
The distribution of snowfall have almost the same patterns across years. The highest peak occured twice at 10 mm and 25 mm. There are also two peaks at around 45 mm and 75 mm.

